#!/usr/bin/env python3
"""
Lab 2: Understanding Embeddings with Real AI Models
Using sentence-transformers to show how text becomes meaningful vectors
"""

import numpy as np
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

def visualize_embedding_slice(embedding, start=0, end=10):
    """Show a small slice of the embedding vector visually"""
    print(f"\nüìä Showing dimensions {start}-{end} of 384 total dimensions:")
    print("  Value:  ", end="")
    for val in embedding[start:end]:
        # Show positive/negative with different symbols
        if val > 0.5:
            print("‚ñ≤‚ñ≤", end=" ")  # Strong positive
        elif val > 0:
            print("‚ñ≤ ", end=" ")  # Positive
        elif val < -0.5:
            print("‚ñº‚ñº", end=" ")  # Strong negative
        else:
            print("‚ñº ", end=" ")  # Negative
    print()
    print("  Number: ", end="")
    for val in embedding[start:end]:
        print(f"{val:+.1f}", end=" ")
    print()

def cosine_similarity(vec1, vec2):
    """Calculate cosine similarity between two vectors"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

def main():
    print("=" * 70)
    print("ü§ñ LAB 2: REAL AI EMBEDDINGS")
    print("=" * 70)
    print("\nLet's use a real AI model (all-MiniLM-L6-v2) to convert text to vectors!")
    print("This model creates 384-dimensional vectors that capture meaning.\n")
    
    print("Loading AI model (this takes a few seconds)...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("‚úÖ Model loaded!\n")
    
    input("‚û°Ô∏è  Press Enter to see the magic of embeddings...")
    
    # STEP 1: Show that similar words create similar embeddings
    print("\n" + "=" * 70)
    print("STEP 1: SIMILAR WORDS ‚Üí SIMILAR NUMBERS")
    print("=" * 70)
    
    word_pairs = [
        ("holiday", "vacation"),          # From narration - key example!
        ("dress code", "clothing rules"),
        ("jeans", "denim"),
        ("Friday", "weekend")
    ]
    
    print("\nLet's compare similar concepts:")
    print("-" * 50)
    
    for word1, word2 in word_pairs:
        emb1 = model.encode(word1)
        emb2 = model.encode(word2)
        similarity = cosine_similarity(emb1, emb2)
        
        print(f'\nüìù "{word1}" vs "{word2}"')
        print(f"   Similarity: {similarity:.1%}")
        
        # Show a tiny slice of the vectors
        print(f'   "{word1}" vector (first 5 values): [{", ".join(f"{v:.2f}" for v in emb1[:5])}...]')
        print(f'   "{word2}" vector (first 5 values): [{", ".join(f"{v:.2f}" for v in emb2[:5])}...]')
    
    print("\nüí° Notice how 'holiday' and 'vacation' - completely different words -")
    print("   have high similarity because the AI understands they mean similar things!")
    
    # NEW: Add dimensionality explanation
    print("\n" + "=" * 70)
    print("üìè WHY 384 DIMENSIONS?")
    print("=" * 70)
    print("""
Why do we need so many dimensions? Each dimension captures different aspects:
‚Ä¢ Tone (formal vs casual)
‚Ä¢ Context (work vs personal)
‚Ä¢ Formality level
‚Ä¢ Semantic category
‚Ä¢ And hundreds more subtle features!

Think of it like describing a person:
‚Ä¢ 1D: Just height (not enough!)
‚Ä¢ 2D: Height + weight (better)
‚Ä¢ 384D: Everything from personality to preferences (complete picture!)

Industry standard: 768-1536 dimensions for production systems.
Our model (all-MiniLM-L6-v2): 384 dimensions - perfect balance!
""")
    
    input("\n‚û°Ô∏è  Press Enter to see the burden shift...")
    
    # NEW: Add burden shift discussion
    print("\n" + "=" * 70)
    print("‚öñÔ∏è THE TRADEOFF: WHO DOES THE WORK?")
    print("=" * 70)
    print("""
SQL Database:
‚ùå Burden on the SEARCHER
   ‚Ä¢ Must know exact keywords
   ‚Ä¢ Must try multiple variations
   ‚Ä¢ "clothing" won't find "dress code"
   
Vector Database:
‚úÖ Burden on the SETUP (that's you!)
   ‚Ä¢ You configure embeddings
   ‚Ä¢ You set similarity thresholds
   ‚Ä¢ You handle chunking
   
But the result?
üéØ Natural language "just works" for users!
   ‚Ä¢ "What should I wear?" finds dress code policy
   ‚Ä¢ "Holiday time off" finds vacation policy
   ‚Ä¢ No training needed for users!
""")
    
    input("\n‚û°Ô∏è  Press Enter to solve Lab 1's search problem...")
    
    # STEP 2: Solve the original search problem
    print("\n" + "=" * 70)
    print("STEP 2: SOLVING THE SEARCH PROBLEM FROM LAB 1")
    print("=" * 70)
    
    # The policy from our database
    policy = "Business casual attire required Monday through Thursday. Jeans permitted on Fridays."
    policy_embedding = model.encode(policy)
    
    print(f'\nüìÑ Company Policy: "{policy}"')
    print(f"üî¢ Converted to 384-dimensional vector")
    visualize_embedding_slice(policy_embedding, 0, 10)
    
    # Test queries with varying relevance
    queries = [
        "Can I wear jeans on Friday?",        # Very relevant - direct match
        "What's the dress code policy?",      # Relevant - about dress code
        "When can I take vacation?",          # Not relevant - different topic
        "How's the weather today?"            # Completely unrelated
    ]
    
    print("\n" + "=" * 50)
    print("Testing employee questions with varying relevance:")
    print("=" * 50)
    print("Watch how the AI accurately scores relevance!")
    
    for query in queries:
        query_embedding = model.encode(query)
        similarity = cosine_similarity(policy_embedding, query_embedding)
        
        print(f'\n‚ùì "{query}"')
        print(f"   Similarity to dress code policy: {similarity:.1%}")
        
        if similarity > 0.5:
            print("   ‚úÖ STRONG MATCH - Highly relevant to dress code!")
        elif similarity > 0.3:
            print("   üîó PARTIAL MATCH - Somewhat related")
        elif similarity > 0.2:
            print("   ‚ö†Ô∏è  WEAK MATCH - Different topic")
        else:
            print("   ‚ùå NO MATCH - Completely unrelated")
    
    print("\nüí° Notice how the AI correctly identifies:")
    print("   ‚Ä¢ 'Jeans on Friday' ‚Üí High similarity (directly mentioned in policy)")
    print("   ‚Ä¢ 'Dress code policy' ‚Üí Good similarity (same topic)")
    print("   ‚Ä¢ 'Vacation' ‚Üí Lower similarity (different HR topic)")
    print("   ‚Ä¢ 'Weather' ‚Üí Very low similarity (completely unrelated)")
    
    input("\n‚û°Ô∏è  Press Enter to try the interactive demo...")
    
    # STEP 3: Interactive comparison
    print("\n" + "=" * 70)
    print("üéÆ INTERACTIVE: COMPARE ANY TWO TEXTS")
    print("=" * 70)
    print("\nType two texts and see how similar the AI thinks they are!")
    
    while True:
        text1 = input("\nüìù Enter first text: ")
        text2 = input("üìù Enter second text: ")
        
        if text1 and text2:
            emb1 = model.encode(text1)
            emb2 = model.encode(text2)
            similarity = cosine_similarity(emb1, emb2)
            
            print(f"\nüîç Comparing:")
            print(f'   Text 1: "{text1}"')
            print(f'   Text 2: "{text2}"')
            print(f"\nüìä Results:")
            print(f"   Similarity: {similarity:.1%}")
            
            if similarity > 0.8:
                print("   üéØ Very similar meaning!")
            elif similarity > 0.5:
                print("   ‚úÖ Related topics")
            elif similarity > 0.3:
                print("   üîó Somewhat related")
            else:
                print("   ‚ùå Different topics")
            
            # Show vector comparison
            print("\nVector comparison (first 10 dimensions):")
            print("Text 1:", end="")
            visualize_embedding_slice(emb1, 0, 10)
            print("Text 2:", end="")
            visualize_embedding_slice(emb2, 0, 10)
        
        # Ask if they want to try another
        another = input("\nüîÑ Would you like to try another comparison? (y/n): ")
        if another.lower() not in ['y', 'yes']:
            break
    
    # Summary
    print("\n" + "=" * 70)
    print("üéì WHAT WE LEARNED")
    print("=" * 70)
    print("""
What you discovered:
1. Real AI models convert text to 384-dimensional vectors
2. Similar meanings ‚Üí Similar vectors (even with different words!)
3. "dress code" and "clothing rules" have high similarity despite ZERO shared words
4. AI accurately scores relevance: dress code queries rank high, weather ranks low

This is the foundation of semantic search!
Next, we'll implement similarity search to find relevant documents.
""")
    
    # Save completion marker
    with open('lab2_embeddings_demo.txt', 'w') as f:
        f.write("Lab 2 completed: Real AI Embeddings\n")
        f.write("Model used: all-MiniLM-L6-v2 (384 dimensions)\n")
        f.write("Key Learning: Real embeddings capture semantic meaning\n")
    
    print("‚úÖ Lab 2 Complete! Ready for Lab 3: Similarity Search")

if __name__ == "__main__":
    main()