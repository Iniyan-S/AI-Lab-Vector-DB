#!/usr/bin/env python3
"""
Lab 4: Production Vector Database with ChromaDB
Real documents, real embeddings, real search!
"""

import os
import chromadb
from chromadb.utils import embedding_functions
from datetime import datetime

def load_documents_from_folder(folder_path):
    """Load all markdown documents from a folder"""
    documents = []
    
    if not os.path.exists(folder_path):
        print(f"âŒ Folder {folder_path} not found!")
        return documents
    
    for filename in os.listdir(folder_path):
        if filename.endswith('.md'):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r') as f:
                content = f.read()
                # Remove the .md extension for display
                doc_name = filename.replace('.md', '').replace('_', ' ').title()
                documents.append({
                    'content': content,
                    'source': filename,
                    'title': doc_name
                })
    
    return documents

def split_into_sentences(text):
    """Split text into sentences using punctuation markers"""
    import re
    # Split on sentence endings but keep the punctuation
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]

def split_into_paragraphs(text):
    """Split text into paragraphs based on double newlines or markdown headers"""
    import re
    # Split on double newlines or markdown headers
    paragraphs = re.split(r'\n\n+|(?=^#{1,6}\s)', text, flags=re.MULTILINE)
    return [p.strip() for p in paragraphs if p.strip()]

def smart_chunk_document(text, source, chunk_size=500, overlap_sentences=2):
    """
    Intelligent chunking that respects sentence and paragraph boundaries.
    Never splits words or sentences in the middle.
    
    Args:
        text: Document text to chunk
        source: Source document name
        chunk_size: Target chunk size in characters
        overlap_sentences: Number of sentences to overlap between chunks
    """
    chunks = []
    chunk_id = 0
    
    # First try paragraph-based chunking
    paragraphs = split_into_paragraphs(text)
    
    current_chunk = []
    current_size = 0
    
    for paragraph in paragraphs:
        para_size = len(paragraph)
        
        # If a single paragraph is too large, split it by sentences
        if para_size > chunk_size:
            sentences = split_into_sentences(paragraph)
            
            for sentence in sentences:
                sent_size = len(sentence)
                
                # If adding this sentence exceeds chunk size, save current chunk
                if current_size + sent_size > chunk_size and current_chunk:
                    chunk_text = ' '.join(current_chunk)
                    chunks.append({
                        'text': chunk_text,
                        'metadata': {
                            'source': source,
                            'chunk_id': chunk_id,
                            'sentence_count': len(current_chunk)
                        }
                    })
                    chunk_id += 1
                    
                    # Keep last N sentences for overlap
                    if overlap_sentences > 0 and len(current_chunk) >= overlap_sentences:
                        current_chunk = current_chunk[-overlap_sentences:]
                        current_size = sum(len(s) + 1 for s in current_chunk)
                    else:
                        current_chunk = []
                        current_size = 0
                
                # Add sentence to current chunk
                current_chunk.append(sentence)
                current_size += sent_size + 1
        
        # If paragraph fits, add it whole
        elif current_size + para_size > chunk_size and current_chunk:
            # Save current chunk
            chunk_text = ' '.join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'metadata': {
                    'source': source,
                    'chunk_id': chunk_id,
                    'sentence_count': len(current_chunk)
                }
            })
            chunk_id += 1
            
            # Start new chunk with overlap
            if overlap_sentences > 0 and len(current_chunk) >= overlap_sentences:
                # For overlap, get last N sentences from the chunk
                all_sentences = []
                for item in current_chunk:
                    all_sentences.extend(split_into_sentences(item))
                if len(all_sentences) >= overlap_sentences:
                    current_chunk = all_sentences[-overlap_sentences:]
                    current_size = sum(len(s) + 1 for s in current_chunk)
                else:
                    current_chunk = [paragraph]
                    current_size = para_size
            else:
                current_chunk = [paragraph]
                current_size = para_size
        else:
            # Add paragraph to current chunk
            current_chunk.append(paragraph)
            current_size += para_size + 1
    
    # Don't forget the last chunk
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        chunks.append({
            'text': chunk_text,
            'metadata': {
                'source': source,
                'chunk_id': chunk_id,
                'sentence_count': len(current_chunk)
            }
        })
    
    # Add total chunks to metadata
    for chunk in chunks:
        chunk['metadata']['total_chunks'] = len(chunks)
    
    return chunks

def setup_chromadb():
    """Initialize ChromaDB with sentence-transformers embedding"""
    print("ğŸ”§ Initializing ChromaDB with real embeddings...")
    
    # Use persistent storage
    client = chromadb.PersistentClient(path="./chroma_db")
    
    # Delete existing collection for clean demo
    try:
        client.delete_collection(name="company_docs")
    except:
        pass
    
    # Create collection with sentence-transformers embedding
    # This uses the all-MiniLM-L6-v2 model (384 dimensions)
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="all-MiniLM-L6-v2"
    )
    
    collection = client.create_collection(
        name="company_docs",
        embedding_function=sentence_transformer_ef,
        metadata={"hnsw:space": "cosine"}
    )
    
    print("âœ… ChromaDB initialized with all-MiniLM-L6-v2 embeddings (384 dimensions)")
    return client, collection

def load_and_chunk_documents(folder_path, collection):
    """Load documents, chunk them, and add to ChromaDB"""
    print("\nğŸ“š Loading and processing company documents...")
    print("-" * 50)
    
    documents = load_documents_from_folder(folder_path)
    
    if not documents:
        print("âŒ No documents found!")
        return 0
    
    total_chunks = 0
    all_chunks = []
    all_metadatas = []
    all_ids = []
    
    for doc in documents:
        print(f"\nğŸ“„ Processing: {doc['title']}")
        print(f"   Size: {len(doc['content'])} characters")
        
        # Chunk the document with sentence-based overlap
        chunks = smart_chunk_document(
            doc['content'], 
            doc['source'],
            chunk_size=500,
            overlap_sentences=2  # Overlap 2 complete sentences
        )
        
        print(f"   Created {len(chunks)} chunks")
        
        # Prepare for ChromaDB
        for chunk in chunks:
            chunk_id = f"{doc['source']}_{chunk['metadata']['chunk_id']}"
            
            all_chunks.append(chunk['text'])
            all_ids.append(chunk_id)
            all_metadatas.append({
                'source': doc['source'],
                'title': doc['title'],
                'chunk_id': chunk['metadata']['chunk_id'],
                'total_chunks': len(chunks)
            })
        
        total_chunks += len(chunks)
    
    # Add all chunks to ChromaDB at once
    if all_chunks:
        print(f"\nğŸ”„ Adding {total_chunks} chunks to ChromaDB...")
        collection.add(
            documents=all_chunks,
            metadatas=all_metadatas,
            ids=all_ids
        )
        print(f"âœ… Successfully indexed {total_chunks} chunks from {len(documents)} documents")
    
    return total_chunks

def search_documents(collection, query, n_results=3):
    """Search across all document chunks"""
    results = collection.query(
        query_texts=[query],
        n_results=n_results
    )
    
    formatted_results = []
    if results['documents'] and results['documents'][0]:
        for i in range(len(results['documents'][0])):
            formatted_results.append({
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i],
                'similarity': 1 - results['distances'][0][i]
            })
    
    return formatted_results

def main():
    """Production vector database with real documents"""
    print("=" * 70)
    print("ğŸš€ Lab 4: Production Vector Database with ChromaDB")
    print("=" * 70)
    print("\nBuilding a REAL semantic search system with actual documents!")
    
    # Setup ChromaDB with real embeddings
    client, collection = setup_chromadb()
    
    # Load and process documents
    docs_folder = "./docs"
    total_chunks = load_and_chunk_documents(docs_folder, collection)
    
    if total_chunks == 0:
        print("\nâŒ No documents to search!")
        return
    
    # Test with predefined queries
    print("\n" + "=" * 70)
    print("ğŸ” TESTING SEMANTIC SEARCH")
    print("=" * 70)
    
    test_queries = [
        "Can I wear jeans on Monday?",
        "How many vacation days do I get?",
        "What's the work from home policy?",
        "Do I need VPN for remote work?",
        "How much is the parking pass?",
        "What's the expense limit without receipts?",
        "When are performance reviews?",
        "What's the 401k match?"
    ]
    
    print("\nTesting with common employee questions:\n")
    
    for query in test_queries:
        print(f"â“ Question: '{query}'")
        print("-" * 50)
        
        results = search_documents(collection, query, n_results=1)
        
        if results:
            best = results[0]
            print(f"ğŸ“ Found in: {best['metadata']['title']}")
            print(f"   Chunk {best['metadata']['chunk_id'] + 1} of {best['metadata']['total_chunks']}")
            print(f"   Similarity: {best['similarity']:.1%}")
            
            # Visual similarity bar
            bar_length = int(best['similarity'] * 30)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
            print(f"   [{bar}]")
            
            # Show relevant answer from the chunk
            chunk_text = best['text'].strip()
            # Find the most relevant sentence that likely answers the question
            sentences = split_into_sentences(chunk_text)
            
            # Show first 2 sentences or 200 chars, whichever is shorter
            if sentences:
                answer_preview = '. '.join(sentences[:2]) + '.'
                if len(answer_preview) > 200:
                    answer_preview = chunk_text[:200] + "..."
            else:
                answer_preview = chunk_text[:200] + "..."
            
            print(f"\n   ğŸ“ Answer: \"{answer_preview}\"")
        else:
            print("   âŒ No relevant information found")
        
        print()
    
    # Interactive search
    print("\n" + "=" * 70)
    print("ğŸ® INTERACTIVE SEARCH - Try Your Own Questions!")
    print("=" * 70)
    print("\nAsk any question about company policies.")
    print("Type 'done' to finish.\n")
    
    while True:
        user_query = input("ğŸ” Your question: ").strip()
        
        if user_query.lower() in ['done', 'quit', 'exit', '']:
            break
        
        results = search_documents(collection, user_query, n_results=3)
        
        if results:
            # Show the best match with full content
            best_result = results[0]
            print(f"\nğŸ¯ BEST MATCH FOUND:")
            print("=" * 50)
            print(f"ğŸ“„ Document: {best_result['metadata']['title']}")
            print(f"ğŸ“ Location: Chunk {best_result['metadata']['chunk_id'] + 1} of {best_result['metadata']['total_chunks']}")
            print(f"ğŸ” Similarity: {best_result['similarity']:.1%}")
            
            # Visual similarity bar
            bar_length = int(best_result['similarity'] * 30)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
            print(f"[{bar}]")
            
            print("\nğŸ“ RELEVANT CONTENT:")
            print("-" * 50)
            # Display the full chunk content formatted nicely
            chunk_text = best_result['text'].strip()
            # Wrap long lines for better readability
            import textwrap
            wrapped_text = textwrap.fill(chunk_text, width=70, initial_indent="", subsequent_indent="")
            print(wrapped_text)
            print("-" * 50)
            
            # Show other relevant results briefly
            if len(results) > 1:
                print(f"\nğŸ“š Also found {len(results)-1} other relevant chunks:")
                for i, result in enumerate(results[1:], 2):
                    print(f"   {i}. {result['metadata']['title']} (Chunk {result['metadata']['chunk_id']+1}, {result['similarity']:.1%} match)")
            print()
        else:
            print("\nâŒ No relevant information found. Try rephrasing your question.\n")
    
    # Show chunking demonstration
    print("\n" + "=" * 70)
    print("ğŸ“„ INTELLIGENT CHUNKING DEMONSTRATION")
    print("=" * 70)
    
    sample_text = """Employees receive 15 vacation days annually. Vacation requests must be submitted two weeks in advance. Holiday time requires manager approval. Remote work is permitted up to 3 days per week."""
    
    print("\nğŸ“‹ Sample policy text:")
    print(f"'{sample_text}'")
    print(f"Length: {len(sample_text)} characters")
    
    # Show the problem with naive character chunking
    print("\nâŒ PROBLEM: Naive character chunking (50 chars):")
    print("-" * 50)
    naive_chunks = []
    for i in range(0, len(sample_text), 50):
        chunk = sample_text[i:i+50]
        naive_chunks.append(chunk)
        print(f"Chunk {len(naive_chunks)}: '{chunk}'")
        if "vacat" in chunk.lower() and "ion" not in chunk.lower():
            print("         âš ï¸  'vacation' is split!")
    
    # Show intelligent sentence-based chunking
    print("\nâœ… SOLUTION: Sentence-based chunking:")
    print("-" * 50)
    sentences = split_into_sentences(sample_text)
    print(f"Sentences detected: {len(sentences)}")
    for i, sentence in enumerate(sentences, 1):
        print(f"Sentence {i}: '{sentence}'")
    
    print("\nğŸ“¦ Creating chunks with 2-sentence overlap:")
    smart_chunks = smart_chunk_document(sample_text, "demo.md", chunk_size=100, overlap_sentences=2)
    for i, chunk in enumerate(smart_chunks, 1):
        print(f"\nChunk {i}: '{chunk['text']}'")
        if 'sentence_count' in chunk['metadata']:
            print(f"         ğŸ“Š Contains {chunk['metadata']['sentence_count']} complete sentences")
    
    print("\nâœ… Benefits of intelligent chunking:")
    print("   â€¢ Never splits words (no 'vaca|tion')")
    print("   â€¢ Preserves complete sentences")
    print("   â€¢ Maintains semantic meaning")
    print("   â€¢ Overlap uses full sentences, not arbitrary characters")
    
    # Summary statistics
    print("\n" + "=" * 70)
    print("ğŸ“Š VECTOR DATABASE STATISTICS")
    print("=" * 70)
    
    print(f"""
âœ… System Performance:
   â€¢ Documents processed: 4
   â€¢ Total chunks created: {total_chunks}
   â€¢ Embedding dimensions: 384 (all-MiniLM-L6-v2)
   â€¢ Storage: Persistent ChromaDB
   â€¢ Search method: Cosine similarity
   
ğŸ¯ Key Achievements:
   â€¢ Real documents with actual company policies
   â€¢ Production-grade embeddings (not toy examples)
   â€¢ Semantic search across multiple documents
   â€¢ Chunk-level precision with source tracking
   â€¢ Interactive natural language queries
   
ğŸ’¡ The Power of Vector Databases:
   â€¢ Find information using natural language
   â€¢ No need for exact keyword matches
   â€¢ Understands context and synonyms
   â€¢ Scales to thousands of documents
   â€¢ Sub-second search performance
""")
    
    # Save completion marker
    with open('lab4_chromadb_vector_database.txt', 'w') as f:
        f.write("Lab 4 completed: Production Vector Database with ChromaDB\n")
        f.write(f"Documents processed: 4\n")
        f.write(f"Total chunks: {total_chunks}\n")
        f.write(f"Embedding model: all-MiniLM-L6-v2 (384 dimensions)\n")
        f.write("Status: Production-ready semantic search system\n")
    
    print("ğŸ‰ Congratulations! You've built a production vector database!")
    print("   Real documents âœ“  Real embeddings âœ“  Real search âœ“")

if __name__ == "__main__":
    main()